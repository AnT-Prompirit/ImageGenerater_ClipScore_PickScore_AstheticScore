{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7af21a93-d3cc-4eb7-9b3a-976b3e434829",
      "metadata": {
        "id": "7af21a93-d3cc-4eb7-9b3a-976b3e434829"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the uploaded CSV file\n",
        "df = pd.read_csv('your directory/dataset.csv')\n",
        "\n",
        "# Add columns\n",
        "columns_to_add = [\"sd_image\", \"sd_ITA\", \"sd_IEA\", \"PickScore\", \"Asthetic\"]\n",
        "for column in columns_to_add:\n",
        "    df[column] = \"\"\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5edda6c-a4f8-46bf-a672-d740cdc0f184",
      "metadata": {
        "scrolled": true,
        "id": "b5edda6c-a4f8-46bf-a672-d740cdc0f184"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers\n",
        "!pip install torchmetrics\n",
        "!pip install accelerate\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install open-clip-torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb4df54-5c18-4db7-adeb-5bc0c7511bb4",
      "metadata": {
        "id": "5fb4df54-5c18-4db7-adeb-5bc0c7511bb4"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "import torch\n",
        "from torchmetrics.functional.multimodal import clip_score\n",
        "from functools import partial\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "from huggingface_hub import notebook_login\n",
        "import IPython.display as display\n",
        "import cv2\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "import clip\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from os.path import expanduser\n",
        "from urllib.request import urlretrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ba4eeb-a229-4b23-8360-3f694fd3ca74",
      "metadata": {
        "scrolled": true,
        "id": "58ba4eeb-a229-4b23-8360-3f694fd3ca74"
      },
      "outputs": [],
      "source": [
        "# Required to get access to stable diffusion model\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and Initialize Stable Diffusion model\n",
        "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Function to convert NumPy array to Base64 encoded string\n",
        "def numpy_to_base64(image_np):\n",
        "    image_pil = Image.fromarray(image_np.astype(\"uint8\"), \"RGB\")\n",
        "    buffered = BytesIO()\n",
        "    image_pil.save(buffered, format=\"JPEG\")\n",
        "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
        "    return img_str\n",
        "\n",
        "# Function to generate image from text prompt\n",
        "def generate_image(prompt):\n",
        "    with torch.no_grad():\n",
        "        generated = pipe(prompt).images[0]\n",
        "    return np.array(generated)\n",
        "\n",
        "# Calculate CLIP score\n",
        "clip_score_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
        "def calculate_clip_score(image, prompt):\n",
        "    images_int = (image * 255).astype(\"uint8\")\n",
        "    clip_score = clip_score_fn(torch.from_numpy(images_int).unsqueeze(0).permute(0, 3, 1, 2), [prompt]).detach()\n",
        "    return round(float(clip_score), 4)\n",
        "\n",
        "# Import and Initialize PickScore model\n",
        "processor = AutoProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
        "model = AutoModel.from_pretrained(\"yuvalkirstain/PickScore_v1\").eval().to(device)\n",
        "\n",
        "# Calculate PickScore\n",
        "def calc_prob(prompt, image):\n",
        "    # preprocess\n",
        "    image_input = processor(images=image, padding=True, truncation=True, max_length=77, return_tensors=\"pt\").to(device)\n",
        "    text_input = processor(text=prompt, padding=True, truncation=True, max_length=77, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        # embed\n",
        "        image_emb = model.get_image_features(**image_input)\n",
        "        image_emb = image_emb / torch.norm(image_emb, dim=-1, keepdim=True)\n",
        "        text_emb = model.get_text_features(**text_input)\n",
        "        text_emb = text_emb / torch.norm(text_emb, dim=-1, keepdim=True)\n",
        "        # score\n",
        "        score = model.logit_scale.exp() * (text_emb @ image_emb.T)[0]\n",
        "    return score.item()\n",
        "\n",
        "# Asthetic model\n",
        "def get_aesthetic_model(clip_model=\"vit_l_14\"):\n",
        "    home = expanduser(\"~\")\n",
        "    cache_folder = home + \"/.cache/emb_reader\"\n",
        "    path_to_model = cache_folder + \"/sa_0_4_\"+clip_model+\"_linear.pth\"\n",
        "    if not os.path.exists(path_to_model):\n",
        "        os.makedirs(cache_folder, exist_ok=True)\n",
        "        url_model = (\n",
        "            \"https://github.com/LAION-AI/aesthetic-predictor/blob/main/sa_0_4_\"+clip_model+\"_linear.pth?raw=true\"\n",
        "        )\n",
        "        urlretrieve(url_model, path_to_model)\n",
        "    if clip_model == \"vit_l_14\":\n",
        "        m = nn.Linear(768, 1)\n",
        "    elif clip_model == \"vit_b_32\":\n",
        "        m = nn.Linear(512, 1)\n",
        "    else:\n",
        "        raise ValueError()\n",
        "    s = torch.load(path_to_model)\n",
        "    m.load_state_dict(s)\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "# Asthetic Score model initialization\n",
        "amodel = get_aesthetic_model(clip_model=\"vit_l_14\")\n",
        "amodel.eval()\n",
        "\n",
        "# Calculate Asthetic Score\n",
        "def calculate_aesthetic_score(pil_image):\n",
        "    image = preprocess(pil_image).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        image_features = c_model.encode_image(image)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        prediction = amodel(image_features)\n",
        "    return prediction.item()\n",
        "\n",
        "import open_clip\n",
        "c_model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')"
      ],
      "metadata": {
        "id": "NsZe7236KmGq"
      },
      "id": "NsZe7236KmGq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over DataFrame and process data\n",
        "for index, row in df.iterrows():\n",
        "    print(index)\n",
        "    emotion = row['emotion']\n",
        "    utterance = row['utterance']\n",
        "\n",
        "    # Image generation\n",
        "    image = generate_image(utterance)\n",
        "    generated_image_pil = Image.fromarray(image.astype(\"uint8\"), \"RGB\")\n",
        "    df.at[index, 'sd_image'] = numpy_to_base64(image)\n",
        "\n",
        "    # CLIP score calculation\n",
        "    sd_ITA = calculate_clip_score(image, utterance)\n",
        "    sd_IEA = calculate_clip_score(image, emotion)\n",
        "    df.at[index, 'sd_ITA'] = sd_ITA\n",
        "    df.at[index, 'sd_IEA'] = sd_IEA\n",
        "\n",
        "    # PickScore calculation\n",
        "    pick_score = calc_prob(utterance, generated_image_pil)\n",
        "    df.at[index, 'PickScore'] = pick_score\n",
        "\n",
        "    # Asthetic Scorecalculation\n",
        "    aesthetic_score = calculate_aesthetic_score(generated_image_pil)\n",
        "    df.at[index, 'Asthetic'] = aesthetic_score\n",
        "\n",
        "    df.to_csv('your directory/dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "VZgPgrrHLnyT"
      },
      "id": "VZgPgrrHLnyT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean\n",
        "sd_ITA_average = df['sd_ITA'].mean()\n",
        "sd_IEA_average = df['sd_IEA'].mean()\n",
        "PickScore_average = df['PickScore'].mean()\n",
        "Asthetic_average = df['Asthetic'].mean()\n",
        "\n",
        "# median\n",
        "sd_ITA_median = df['sd_ITA'].median()\n",
        "sd_IEA_median = df['sd_IEA'].median()\n",
        "PickScore_median = df['PickScore'].median()\n",
        "Asthetic_median = df['Asthetic'].median()\n",
        "\n",
        "# std\n",
        "sd_ITA_std = df['sd_ITA'].std()\n",
        "sd_IEA_std = df['sd_IEA'].std()\n",
        "PickScore_std = df['PickScore'].std()\n",
        "Asthetic_std = df['Asthetic'].std()\n",
        "\n",
        "# print result\n",
        "print(\"sd_ITA 평균:\", sd_ITA_average)\n",
        "print(\"sd_IEA 평균:\", sd_IEA_average)\n",
        "print(\"PickScore 평균:\", PickScore_average)\n",
        "print(\"Asthetic 평균:\", Asthetic_average)\n",
        "\n",
        "print(\"sd_ITA 중앙값:\", sd_ITA_median)\n",
        "print(\"sd_IEA 중앙값:\", sd_IEA_median)\n",
        "print(\"PickScore 중앙값:\", PickScore_median)\n",
        "print(\"Asthetic 중앙값:\", Asthetic_median)\n",
        "\n",
        "print(\"sd_ITA 표준편차:\", sd_ITA_std)\n",
        "print(\"sd_IEA 표준편차:\", sd_IEA_std)\n",
        "print(\"PickScore 중앙값:\", PickScore_std)\n",
        "print(\"Asthetic 중앙값:\", Asthetic_std)"
      ],
      "metadata": {
        "id": "DfH-nPd47F-C"
      },
      "id": "DfH-nPd47F-C",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}